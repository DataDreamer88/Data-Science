# Is there a correlation between Time on Website & Yearly Amount Spent?
ggplot(data, aes(x=Time.on.Website, y=Yearly.Amount.Spent)) +
geom_point(colour="orange") +
ggtitle("Time on website against vs Yearly amount spent") +
xlab("Time on Website") +
ylab("Yearly Amount Spent")
# Is there a correlation between Avg Session Length & Yearly Amount Spent?
ggplot(data, aes(x=Avg..Session.Length, y=Yearly.Amount.Spent)) +
geom_point(colour="orange") +
ggtitle("Average session length against vs Yearly amount spent") +
xlab("Time on Website") +
ylab("Yearly Amount Spent")
# pairplot of all continuous variables -->
#### length of membership seems the most correlated
pairs(data[c("Avg..Session.Length",
"Time.on.App",
"Time.on.Website",
"Length.of.Membership",
"Yearly.Amount.Spent")],
col = "orange",
pch = 16,
labels = c("Avg Session Length",
"Time on App",
"Time on website",
"Length of Membership",
"Yearly spent"),
main = "Pairplot of variables")
# ---------------------------------------------------------------------------
# EXPLORING THE SELECTED VARIABLES
# is the variable normally distributed?
hist(data$Length.of.Membership)
# with ggplot
ggplot(data, aes(x=Length.of.Membership)) +
geom_histogram(
color= "white",
fill="orange",
binwidth = 0.5)
# check distribution with boxplot
boxplot(data$Length.of.Membership)
# with ggplot
ggplot(data, aes(x=Length.of.Membership)) +
geom_boxplot(
fill="orange",
)
# ---------------------------------------------------------------------------
# FITTING A LINEAR MODEL
attach(data)
lm.fit1 <- lm(Yearly.Amount.Spent~Length.of.Membership)
summary(lm.fit1)
# --> p value is below significance level, so the variable Length of
#     membership is significant.
# --> beta_1 is 64.219, which means that an increase in the variable value
#     causes an increase in the target variable.
plot(Yearly.Amount.Spent~Length.of.Membership)
abline(lm.fit1, col="red")
# ---------------------------------------------------------------------------
# RESIDUALS ANALYSIS
qqnorm(residuals(lm.fit1))
qqline(residuals(lm.fit1), col="red")
shapiro.test(residuals(lm.fit1))
# --> the p value is > 0.05 so Ho cannot be rejected. The residuals
#     distribution is normal.
# --> normality of residuals is an assumption of the linear model.
#     this means that the chosen model works correctly.
# ---------------------------------------------------------------------------
# EVALUATE THE QUALITY OF THE MODEL
# create a random training and a testing set
set.seed(1)
row.number <- sample(1:nrow(data), 0.8*nrow(data))
train <- data[row.number,]
test <- data[-row.number,]
# estimate the linear fit with the training set
lm.fit0.8 <- lm(Yearly.Amount.Spent~Length.of.Membership, data=train)
summary(lm.fit0.8)
# predict on testing set
prediction0.8 <- predict(lm.fit0.8, newdata = test)
err0.8 <- prediction0.8 - test$Yearly.Amount.Spent
rmse <- sqrt(mean(err0.8^2))
mape <- mean(abs(err0.8/test$Yearly.Amount.Spent))
c(RMSE=rmse,mape=mape,R2=summary(lm.fit0.8)$r.squared) # to print the 3 parameters
# ---------------------------------------------------------------------------
# MULTIPLE REGRESSION
attach(data)
lm.fit <- lm(Yearly.Amount.Spent~Avg..Session.Length +
Time.on.App +
Time.on.Website +
Length.of.Membership)
summary(lm.fit)
# --------------
# findings :
# 3 of the 4 variables studied seem to have an positive impact on the
# response variable. the most important remains length of membership, with
# a coefficient 1.5 and 2.4 higher than Time on App and Avg Session Length
# respectively.
# Time on website seems to have little impact in the response.
# ---------------------------------------------------------------------------
# EVALUATE THE MULTIPLE REGRESSION MODEL
# create a random training and a testing set
set.seed(1)
row.number <- sample(1:nrow(data), 0.8*nrow(data))
train <- data[row.number,]
test <- data[-row.number,]
# estimate the linear fit with the training set
multi.lm.fit0.8 <- lm(Yearly.Amount.Spent~Avg..Session.Length +
Time.on.App +
Time.on.Website +
Length.of.Membership,
data=train)
summary(multi.lm.fit0.8)
# predict on testing set
prediction.multi0.8 <- predict(multi.lm.fit0.8, newdata = test)
err0.8 <- prediction.multi0.8 - test$Yearly.Amount.Spent
rmse <- sqrt(mean(err0.8^2))
mape <- mean(abs(err0.8/test$Yearly.Amount.Spent))
c(RMSE=rmse,mape=mape,R2=summary(lm.fit0.8)$r.squared) # to print the 3 parameters
# --------
# findings
# by using a multiple linear model, we have created a much more accurate
# predictor of the response variable. R2 went from 0.65 to 0.98
# and the RSE went from 47.14 to 9.97 dollars.
# IMPORT DATA AND SETUP
data <- read.csv("customer-data.txt")
str(data)
summary(data)
# ---------------------------------------------------------------------------
# CREATE PLOTS AND SEARCH FOR CORRELATIONS
library(ggplot2)
# Is there a correlation between Time on Website & Yearly Amount Spent?
ggplot(data, aes(x=Time.on.Website, y=Yearly.Amount.Spent)) +
geom_point(colour="orange") +
ggtitle("Time on website against vs Yearly amount spent") +
xlab("Time on Website") +
ylab("Yearly Amount Spent")
# Is there a correlation between Avg Session Length & Yearly Amount Spent?
ggplot(data, aes(x=Avg..Session.Length, y=Yearly.Amount.Spent)) +
geom_point(colour="orange") +
ggtitle("Average session length against vs Yearly amount spent") +
xlab("Time on Website") +
ylab("Yearly Amount Spent")
# Is there a correlation between Avg Session Length & Yearly Amount Spent?
ggplot(data, aes(x=Avg..Session.Length, y=Yearly.Amount.Spent)) +
geom_point(colour="orange") +
ggtitle("Average session length against vs Yearly amount spent") +
xlab("Time on Website") +
ylab("Yearly Amount Spent")
# pairplot of all continuous variables -->
#### length of membership seems the most correlated
pairs(data[c("Avg..Session.Length",
"Time.on.App",
"Time.on.Website",
"Length.of.Membership",
"Yearly.Amount.Spent")],
col = "orange",
pch = 16,
labels = c("Avg Session Length",
"Time on App",
"Time on website",
"Length of Membership",
"Yearly spent"),
main = "Pairplot of variables")
# ---------------------------------------------------------------------------
# EXPLORING THE SELECTED VARIABLES
# is the variable normally distributed?
hist(data$Length.of.Membership)
# with ggplot
ggplot(data, aes(x=Length.of.Membership)) +
geom_histogram(
color= "white",
fill="orange",
binwidth = 0.5)
# check distribution with boxplot
boxplot(data$Length.of.Membership)
# with ggplot
ggplot(data, aes(x=Length.of.Membership)) +
geom_boxplot(
fill="orange",
)
# ---------------------------------------------------------------------------
# FITTING A LINEAR MODEL
attach(data)
lm.fit1 <- lm(Yearly.Amount.Spent~Length.of.Membership)
summary(lm.fit1)
# --> p value is below significance level, so the variable Length of
#     membership is significant.
# --> beta_1 is 64.219, which means that an increase in the variable value
#     causes an increase in the target variable.
plot(Yearly.Amount.Spent~Length.of.Membership)
abline(lm.fit1, col="red")
# ---------------------------------------------------------------------------
# RESIDUALS ANALYSIS
qqnorm(residuals(lm.fit1))
qqline(residuals(lm.fit1), col="red")
shapiro.test(residuals(lm.fit1))
# --> the p value is > 0.05 so Ho cannot be rejected. The residuals
#     distribution is normal.
# --> normality of residuals is an assumption of the linear model.
#     this means that the chosen model works correctly.
# ---------------------------------------------------------------------------
# EVALUATE THE QUALITY OF THE MODEL
# create a random training and a testing set
set.seed(1)
row.number <- sample(1:nrow(data), 0.8*nrow(data))
train <- data[row.number,]
test <- data[-row.number,]
# estimate the linear fit with the training set
lm.fit0.8 <- lm(Yearly.Amount.Spent~Length.of.Membership, data=train)
summary(lm.fit0.8)
# predict on testing set
prediction0.8 <- predict(lm.fit0.8, newdata = test)
err0.8 <- prediction0.8 - test$Yearly.Amount.Spent
rmse <- sqrt(mean(err0.8^2))
mape <- mean(abs(err0.8/test$Yearly.Amount.Spent))
c(RMSE=rmse,mape=mape,R2=summary(lm.fit0.8)$r.squared) # to print the 3 parameters
# ---------------------------------------------------------------------------
# MULTIPLE REGRESSION
attach(data)
lm.fit <- lm(Yearly.Amount.Spent~Avg..Session.Length +
Time.on.App +
Time.on.Website +
Length.of.Membership)
summary(lm.fit)
# --------------
# findings :
# 3 of the 4 variables studied seem to have an positive impact on the
# response variable. the most important remains length of membership, with
# a coefficient 1.5 and 2.4 higher than Time on App and Avg Session Length
# respectively.
# Time on website seems to have little impact in the response.
# ---------------------------------------------------------------------------
# EVALUATE THE MULTIPLE REGRESSION MODEL
# create a random training and a testing set
set.seed(1)
row.number <- sample(1:nrow(data), 0.8*nrow(data))
train <- data[row.number,]
test <- data[-row.number,]
# estimate the linear fit with the training set
multi.lm.fit0.8 <- lm(Yearly.Amount.Spent~Avg..Session.Length +
Time.on.App +
Time.on.Website +
Length.of.Membership,
data=train)
summary(multi.lm.fit0.8)
# predict on testing set
prediction.multi0.8 <- predict(multi.lm.fit0.8, newdata = test)
err0.8 <- prediction.multi0.8 - test$Yearly.Amount.Spent
rmse <- sqrt(mean(err0.8^2))
mape <- mean(abs(err0.8/test$Yearly.Amount.Spent))
c(RMSE=rmse,mape=mape,R2=summary(lm.fit0.8)$r.squared) # to print the 3 parameters
# --------
# findings
# by using a multiple linear model, we have created a much more accurate
# predictor of the response variable. R2 went from 0.65 to 0.98
# and the RSE went from 47.14 to 9.97 dollars.
# NOW Z-Testing
# - added advanced statistical tests (Z-Test, ANOVA).
# - performed feature scaling and applied regularization methods (Ridge & Lasso).
# - Evaluated the presence of multicollinearity using VIF.
# - Improved model accuracy with k-fold cross-validation.
# - Enhanced model performance analysis using Ridge regression evaluation on the test set.
# ---------------------------------------------------------------------------
# 1. Z-Test for Hypothesis Testing
# Splitting the Length of Membership into two groups (High vs Low)
median_length <- median(data$Length.of.Membership)
high_length <- subset(data, Length.of.Membership > median_length)
low_length <- subset(data, Length.of.Membership <= median_length)
# Performing Z-test on Yearly Amount Spent for High vs Low Membership Length
z.test <- function(x1, x2){
n1 <- length(x1)
n2 <- length(x2)
mean1 <- mean(x1)
mean2 <- mean(x2)
sd1 <- sd(x1)
sd2 <- sd(x2)
pooled_se <- sqrt(sd1^2/n1 + sd2^2/n2)
z_score <- (mean1 - mean2) / pooled_se
p_value <- 2 * (1 - pnorm(abs(z_score)))
return(c(z_score = z_score, p_value = p_value))
}
z_test_result <- z.test(high_length$Yearly.Amount.Spent, low_length$Yearly.Amount.Spent)
print(z_test_result)
# Interpretation: If p_value < 0.05, there is a statistically significant difference.
# ---------------------------------------------------------------------------
# 2. Feature Scaling (Z-score Normalization)
# Scale the features for better model performance
scaled_data <- as.data.frame(scale(data[, c("Avg..Session.Length",
"Time.on.App",
"Time.on.Website",
"Length.of.Membership")]))
scaled_data$Yearly.Amount.Spent <- data$Yearly.Amount.Spent
# ---------------------------------------------------------------------------
# 3. Regularization (Ridge and Lasso Regression)
library(glmnet)
install.packages("glmnet")
# IMPORT DATA AND SETUP
data <- read.csv("customer-data.txt")
str(data)
summary(data)
# ---------------------------------------------------------------------------
# CREATE PLOTS AND SEARCH FOR CORRELATIONS
library(ggplot2)
# Is there a correlation between Time on Website & Yearly Amount Spent?
ggplot(data, aes(x=Time.on.Website, y=Yearly.Amount.Spent)) +
geom_point(colour="orange") +
ggtitle("Time on website against vs Yearly amount spent") +
xlab("Time on Website") +
ylab("Yearly Amount Spent")
# Is there a correlation between Avg Session Length & Yearly Amount Spent?
ggplot(data, aes(x=Avg..Session.Length, y=Yearly.Amount.Spent)) +
geom_point(colour="orange") +
ggtitle("Average session length against vs Yearly amount spent") +
xlab("Time on Website") +
ylab("Yearly Amount Spent")
# pairplot of all continuous variables -->
#### length of membership seems the most correlated
pairs(data[c("Avg..Session.Length",
"Time.on.App",
"Time.on.Website",
"Length.of.Membership",
"Yearly.Amount.Spent")],
col = "orange",
pch = 16,
labels = c("Avg Session Length",
"Time on App",
"Time on website",
"Length of Membership",
"Yearly spent"),
main = "Pairplot of variables")
# ---------------------------------------------------------------------------
# EXPLORING THE SELECTED VARIABLES
# is the variable normally distributed?
hist(data$Length.of.Membership)
# with ggplot
ggplot(data, aes(x=Length.of.Membership)) +
geom_histogram(
color= "white",
fill="orange",
binwidth = 0.5)
# check distribution with boxplot
boxplot(data$Length.of.Membership)
# with ggplot
ggplot(data, aes(x=Length.of.Membership)) +
geom_boxplot(
fill="orange",
)
# ---------------------------------------------------------------------------
# FITTING A LINEAR MODEL
attach(data)
lm.fit1 <- lm(Yearly.Amount.Spent~Length.of.Membership)
summary(lm.fit1)
# --> p value is below significance level, so the variable Length of
#     membership is significant.
# --> beta_1 is 64.219, which means that an increase in the variable value
#     causes an increase in the target variable.
plot(Yearly.Amount.Spent~Length.of.Membership)
abline(lm.fit1, col="red")
# ---------------------------------------------------------------------------
# RESIDUALS ANALYSIS
qqnorm(residuals(lm.fit1))
qqline(residuals(lm.fit1), col="red")
shapiro.test(residuals(lm.fit1))
# --> the p value is > 0.05 so Ho cannot be rejected. The residuals
#     distribution is normal.
# --> normality of residuals is an assumption of the linear model.
#     this means that the chosen model works correctly.
# ---------------------------------------------------------------------------
# EVALUATE THE QUALITY OF THE MODEL
# create a random training and a testing set
set.seed(1)
row.number <- sample(1:nrow(data), 0.8*nrow(data))
train <- data[row.number,]
test <- data[-row.number,]
# estimate the linear fit with the training set
lm.fit0.8 <- lm(Yearly.Amount.Spent~Length.of.Membership, data=train)
summary(lm.fit0.8)
# predict on testing set
prediction0.8 <- predict(lm.fit0.8, newdata = test)
err0.8 <- prediction0.8 - test$Yearly.Amount.Spent
rmse <- sqrt(mean(err0.8^2))
mape <- mean(abs(err0.8/test$Yearly.Amount.Spent))
c(RMSE=rmse,mape=mape,R2=summary(lm.fit0.8)$r.squared) # to print the 3 parameters
# ---------------------------------------------------------------------------
# MULTIPLE REGRESSION
attach(data)
lm.fit <- lm(Yearly.Amount.Spent~Avg..Session.Length +
Time.on.App +
Time.on.Website +
Length.of.Membership)
summary(lm.fit)
# --------------
# findings :
# 3 of the 4 variables studied seem to have an positive impact on the
# response variable. the most important remains length of membership, with
# a coefficient 1.5 and 2.4 higher than Time on App and Avg Session Length
# respectively.
# Time on website seems to have little impact in the response.
# ---------------------------------------------------------------------------
# EVALUATE THE MULTIPLE REGRESSION MODEL
# create a random training and a testing set
set.seed(1)
row.number <- sample(1:nrow(data), 0.8*nrow(data))
train <- data[row.number,]
test <- data[-row.number,]
# estimate the linear fit with the training set
multi.lm.fit0.8 <- lm(Yearly.Amount.Spent~Avg..Session.Length +
Time.on.App +
Time.on.Website +
Length.of.Membership,
data=train)
summary(multi.lm.fit0.8)
# predict on testing set
prediction.multi0.8 <- predict(multi.lm.fit0.8, newdata = test)
err0.8 <- prediction.multi0.8 - test$Yearly.Amount.Spent
rmse <- sqrt(mean(err0.8^2))
mape <- mean(abs(err0.8/test$Yearly.Amount.Spent))
c(RMSE=rmse,mape=mape,R2=summary(lm.fit0.8)$r.squared) # to print the 3 parameters
# --------
# findings
# by using a multiple linear model, we have created a much more accurate
# predictor of the response variable. R2 went from 0.65 to 0.98
# and the RSE went from 47.14 to 9.97 dollars.
# NOW Z-Testing
# - added advanced statistical tests (Z-Test, ANOVA).
# - performed feature scaling and applied regularization methods (Ridge & Lasso).
# - Evaluated the presence of multicollinearity using VIF.
# - Improved model accuracy with k-fold cross-validation.
# - Enhanced model performance analysis using Ridge regression evaluation on the test set.
library(glmnet)
# ---------------------------------------------------------------------------
# 1. Z-Test for Hypothesis Testing
# Splitting the Length of Membership into two groups (High vs Low)
median_length <- median(data$Length.of.Membership)
high_length <- subset(data, Length.of.Membership > median_length)
low_length <- subset(data, Length.of.Membership <= median_length)
# Performing Z-test on Yearly Amount Spent for High vs Low Membership Length
z.test <- function(x1, x2){
n1 <- length(x1)
n2 <- length(x2)
mean1 <- mean(x1)
mean2 <- mean(x2)
sd1 <- sd(x1)
sd2 <- sd(x2)
pooled_se <- sqrt(sd1^2/n1 + sd2^2/n2)
z_score <- (mean1 - mean2) / pooled_se
p_value <- 2 * (1 - pnorm(abs(z_score)))
return(c(z_score = z_score, p_value = p_value))
}
z_test_result <- z.test(high_length$Yearly.Amount.Spent, low_length$Yearly.Amount.Spent)
print(z_test_result)
# Interpretation: If p_value < 0.05, there is a statistically significant difference.
# ---------------------------------------------------------------------------
# 2. Feature Scaling (Z-score Normalization)
# Scale the features for better model performance
scaled_data <- as.data.frame(scale(data[, c("Avg..Session.Length",
"Time.on.App",
"Time.on.Website",
"Length.of.Membership")]))
scaled_data$Yearly.Amount.Spent <- data$Yearly.Amount.Spent
# ---------------------------------------------------------------------------
# 3. Regularization (Ridge and Lasso Regression)
library(glmnet)
# Preparing data for glmnet (Matrix format)
x <- as.matrix(scaled_data[, -5])
y <- scaled_data$Yearly.Amount.Spent
# Ridge Regression
set.seed(1)
ridge_model <- glmnet(x, y, alpha=0)
cv_ridge <- cv.glmnet(x, y, alpha=0)
ridge_best_lambda <- cv_ridge$lambda.min
ridge_best_lambda
ridge_predictions <- predict(ridge_model, s=ridge_best_lambda, newx=x)
# Lasso Regression
set.seed(1)
lasso_model <- glmnet(x, y, alpha=1)
cv_lasso <- cv.glmnet(x, y, alpha=1)
lasso_best_lambda <- cv_lasso$lambda.min
lasso_best_lambda
lasso_predictions <- predict(lasso_model, s=lasso_best_lambda, newx=x)
# Print R^2 values
ridge_r2 <- cor(y, ridge_predictions)^2
lasso_r2 <- cor(y, lasso_predictions)^2
c(Ridge_R2=ridge_r2, Lasso_R2=lasso_r2)
# ---------------------------------------------------------------------------
# 4. Variance Inflation Factor (VIF)
library(car)
vif_values <- vif(lm.fit)
print(vif_values)
# Interpretation: VIF > 5 indicates high multicollinearity.
# ---------------------------------------------------------------------------
# 5. Cross-validation (k-fold)
library(caret)
set.seed(1)
train_control <- trainControl(method = "cv", number = 10)
cv_model <- train(Yearly.Amount.Spent ~ Avg..Session.Length + Time.on.App + Time.on.Website + Length.of.Membership,
data = data, method = "lm", trControl = train_control)
print(cv_model)
# ---------------------------------------------------------------------------
# 6. ANOVA for Model Significance
anova_result <- anova(multi.lm.fit0.8)
print(anova_result)
# Interpretation: Look at the p-value for each predictor to evaluate significance.
# ---------------------------------------------------------------------------
# Final Model Evaluation with Test Set
# RMSE and MAPE calculation for ridge regression on test set
test_x <- as.matrix(scale(test[, c("Avg..Session.Length",
"Time.on.App",
"Time.on.Website",
"Length.of.Membership")]))
ridge_test_pred <- predict(ridge_model, s=ridge_best_lambda, newx=test_x)
ridge_err <- ridge_test_pred - test$Yearly.Amount.Spent
ridge_rmse <- sqrt(mean(ridge_err^2))
ridge_mape <- mean(abs(ridge_err / test$Yearly.Amount.Spent))
c(Ridge_RMSE=ridge_rmse, Ridge_MAPE=ridge_mape)
